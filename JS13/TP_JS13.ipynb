{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4df0b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9301 - loss: 0.2380\n",
      "Epoch 2/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9692 - loss: 0.1010\n",
      "Epoch 3/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9779 - loss: 0.0707\n",
      "Epoch 4/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9830 - loss: 0.0523\n",
      "Epoch 5/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9857 - loss: 0.0439\n",
      "Epoch 6/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9883 - loss: 0.0360\n",
      "Epoch 7/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9903 - loss: 0.0292\n",
      "Epoch 8/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9917 - loss: 0.0253\n",
      "Epoch 9/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9934 - loss: 0.0209\n",
      "Epoch 10/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9932 - loss: 0.0193\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9779 - loss: 0.0925\n",
      "Akurasi pada data uji: 0.9779\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 1. Load dataset MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalisasi data (0-255 → 0-1)\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# One-hot encoding label\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# 2. Bangun model JST\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),        # Ubah 28x28 jadi vektor 784\n",
    "    Dense(128, activation='relu'),        # Hidden layer 1\n",
    "    Dense(64, activation='relu'),         # Hidden layer 2\n",
    "    Dense(10, activation='softmax')       # Output layer (10 kelas)\n",
    "])\n",
    "\n",
    "# 3. Kompilasi model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 4. Latih model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "# 5. Evaluasi model\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Akurasi pada data uji: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c69dc0",
   "metadata": {},
   "source": [
    "## Coba dengan beberapa parameter lain:\n",
    "\n",
    "Ubah jumlah neuron di hidden layer (misal: 256 dan 128).\n",
    "\n",
    "Tambahkan satu hidden layer lagi.\n",
    "\n",
    "Bandingkan akurasi dan waktu pelatihan.\n",
    "\n",
    "Eksperimen dengan fungsi aktivasi Sigmoid vs ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6044fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ HASIL EKSPEIMEN MNIST =================\n",
      "(A) Ubah Neuron (256,128)     -> Acc: 0.9813 | Loss: 0.0839\n",
      "(B) Tambah 1 Hidden Layer      -> Acc: 0.9785 | Loss: 0.0932\n",
      "(C1) SIGMOID (3 Hidden)        -> Acc: 0.9791 | Loss: 0.0819\n",
      "(C2) RELU (3 Hidden)           -> Acc: 0.9831 | Loss: 0.0765\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# MNIST MLP EXPERIMENT (FULL CODE)\n",
    "# ================================\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# ================================\n",
    "# 1. Load & Preprocessing Dataset\n",
    "# ================================\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalisasi pixel 0-255 menjadi 0-1\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# One-hot encoding pada label\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# ============================================\n",
    "# 2. Fungsi untuk membangun dan evaluasi model\n",
    "# ============================================\n",
    "def build_and_train(model):\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "    loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return acc, loss\n",
    "\n",
    "# ================================\n",
    "# 3. EXPERIMENT A: Ubah jumlah neuron\n",
    "# ================================\n",
    "model_neuron = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "acc_neuron, loss_neuron = build_and_train(model_neuron)\n",
    "\n",
    "# ================================\n",
    "# 4. EXPERIMENT B: Tambah 1 Hidden Layer\n",
    "# ================================\n",
    "model_3layer = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),  # tambahan layer\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "acc_3layer, loss_3layer = build_and_train(model_3layer)\n",
    "\n",
    "# ================================\n",
    "# 5. EXPERIMENT C: Sigmoid vs ReLU\n",
    "# ================================\n",
    "\n",
    "# Model dengan SIGMOID\n",
    "model_sigmoid = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(256, activation='sigmoid'),\n",
    "    Dense(128, activation='sigmoid'),\n",
    "    Dense(64, activation='sigmoid'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "acc_sigmoid, loss_sigmoid = build_and_train(model_sigmoid)\n",
    "\n",
    "# Model dengan ReLU\n",
    "model_relu = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "acc_relu, loss_relu = build_and_train(model_relu)\n",
    "\n",
    "# ================================\n",
    "# 6. Print Perbandingan Hasil\n",
    "# ================================\n",
    "print(\"\\n================ HASIL EKSPEIMEN MNIST =================\")\n",
    "print(f\"(A) Ubah Neuron (256,128)     -> Acc: {acc_neuron:.4f} | Loss: {loss_neuron:.4f}\")\n",
    "print(f\"(B) Tambah 1 Hidden Layer      -> Acc: {acc_3layer:.4f} | Loss: {loss_3layer:.4f}\")\n",
    "print(f\"(C1) SIGMOID (3 Hidden)        -> Acc: {acc_sigmoid:.4f} | Loss: {loss_sigmoid:.4f}\")\n",
    "print(f\"(C2) RELU (3 Hidden)           -> Acc: {acc_relu:.4f} | Loss: {loss_relu:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e0f097",
   "metadata": {},
   "source": [
    "### Ringkasan Singkat Perbandingan MNIST\n",
    "\n",
    "Model awal dengan dua hidden layer menghasilkan akurasi 0.9779. Ketika jumlah neuron diperbesar menjadi 256 dan 128, akurasi meningkat menjadi 0.9813, namun waktu pelatihannya lebih lama karena komputasi bertambah. Penambahan satu hidden layer juga sedikit meningkatkan akurasi menjadi 0.9785, tetapi proses training menjadi lebih berat.\n",
    "\n",
    "Perbandingan fungsi aktivasi menunjukkan bahwa Sigmoid memberikan akurasi lebih rendah dan pelatihan lebih lambat dibanding ReLU. ReLU menghasilkan akurasi tertinggi, yaitu 0.9831 pada tiga hidden layer, serta melatih lebih cepat karena tidak mengalami vanishing gradient.\n",
    "\n",
    "Secara keseluruhan, model dengan tiga hidden layer dan aktivasi ReLU memberikan performa terbaik, baik dari akurasi maupun efisiensi pelatihan.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
