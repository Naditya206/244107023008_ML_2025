{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a9c1660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.31588712086860626\n",
      "Epoch 1000, Loss: 0.24305227526805523\n",
      "Epoch 2000, Loss: 0.17983983913777993\n",
      "Epoch 3000, Loss: 0.06001001231356514\n",
      "Epoch 4000, Loss: 0.025099336454873968\n",
      "Epoch 5000, Loss: 0.014477315506819001\n",
      "Epoch 6000, Loss: 0.009853172063522406\n",
      "Epoch 7000, Loss: 0.007359824262360488\n",
      "Epoch 8000, Loss: 0.005827322804156253\n",
      "Epoch 9000, Loss: 0.004800025435994283\n",
      "Prediksi:\n",
      "[[0.06184057]\n",
      " [0.93151529]\n",
      " [0.93138883]\n",
      " [0.05524612]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dataset XOR\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "# Parameter\n",
    "input_size = 2\n",
    "hidden_size = 2\n",
    "output_size = 1\n",
    "lr = 0.1\n",
    "\n",
    "# Inisialisasi bobot\n",
    "W1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "# Fungsi aktivasi\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Training\n",
    "for epoch in range(10000):\n",
    "    # Forward pass\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    # Hitung error\n",
    "    error = y - a2\n",
    "\n",
    "    # Backpropagation\n",
    "    d_a2 = error * sigmoid_derivative(a2)\n",
    "    d_W2 = np.dot(a1.T, d_a2)\n",
    "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
    "\n",
    "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
    "    d_W1 = np.dot(X.T, d_a1)\n",
    "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
    "\n",
    "    # Update bobot\n",
    "    W1 += lr * d_W1\n",
    "    b1 += lr * d_b1\n",
    "    W2 += lr * d_W2\n",
    "    b2 += lr * d_b2\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean(np.square(error))\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Output akhir\n",
    "print(\"Prediksi:\")\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3524de",
   "metadata": {},
   "source": [
    "Tugas 1:\n",
    "\n",
    "Ubah jumlah neuron hidden layer menjadi 3.\n",
    "\n",
    "Bandingkan hasil loss dengan konfigurasi awal.\n",
    "\n",
    "Tambahkan fungsi aktivasi ReLU dan bandingkan hasil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92b09d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.2406648259241591\n",
      "Epoch 1000, Loss: 0.16711558970074314\n",
      "Epoch 2000, Loss: 0.16681733880868335\n",
      "Epoch 3000, Loss: 0.16675321518269734\n",
      "Epoch 4000, Loss: 0.1667277969489927\n",
      "Epoch 5000, Loss: 0.1667170382653612\n",
      "Epoch 6000, Loss: 0.16670577356636412\n",
      "Epoch 7000, Loss: 0.16670057052745652\n",
      "Epoch 8000, Loss: 0.166691859641251\n",
      "Epoch 9000, Loss: 0.16668857344549162\n",
      "\n",
      "Prediksi Akhir:\n",
      "[[0.33344322]\n",
      " [0.33344322]\n",
      " [0.99148434]\n",
      " [0.33344322]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dataset XOR\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "# Pilih aktivasi ('sigmoid' atau 'relu')\n",
    "activation_type = 'relu'   # ubah ke 'sigmoid' jika ingin membandingkan\n",
    "\n",
    "# Parameter\n",
    "input_size = 2\n",
    "hidden_size = 3\n",
    "output_size = 1\n",
    "lr = 0.1\n",
    "\n",
    "# Inisialisasi bobot\n",
    "W1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "# Fungsi aktivasi\n",
    "def sigmoid(x): return 1 / (1 + np.exp(-x))\n",
    "def sigmoid_derivative(x): return x * (1 - x)\n",
    "\n",
    "def relu(x): return np.maximum(0, x)\n",
    "def relu_derivative(x): return np.where(x > 0, 1, 0)\n",
    "\n",
    "# Pilih aktivasi\n",
    "if activation_type == 'sigmoid':\n",
    "    act = sigmoid; d_act = sigmoid_derivative\n",
    "else:\n",
    "    act = relu; d_act = relu_derivative\n",
    "\n",
    "# Training\n",
    "for epoch in range(10000):\n",
    "    # Forward pass\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = act(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    # Error\n",
    "    error = y - a2\n",
    "\n",
    "    # Backpropagation\n",
    "    d_a2 = error * sigmoid_derivative(a2)\n",
    "    d_W2 = np.dot(a1.T, d_a2)\n",
    "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
    "\n",
    "    d_a1 = np.dot(d_a2, W2.T) * d_act(a1)\n",
    "    d_W1 = np.dot(X.T, d_a1)\n",
    "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
    "\n",
    "    # Update\n",
    "    W1 += lr * d_W1; b1 += lr * d_b1\n",
    "    W2 += lr * d_W2; b2 += lr * d_b2\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean(np.square(error))\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Hasil\n",
    "print(\"\\nPrediksi Akhir:\")\n",
    "print(a2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a34929",
   "metadata": {},
   "source": [
    "Pada percobaan pertama, jaringan menggunakan fungsi aktivasi sigmoid pada layer tersembunyinya dengan dua neuron. Hasil pelatihan menunjukkan bahwa loss turun secara signifikan hingga sekitar 0.0048 dan prediksi akhir sangat mendekati target XOR, seperti `0.06`, `0.93`, dan seterusnya. Hal ini terjadi karena sigmoid mampu memberikan perubahan nilai yang halus dan non-linear pada rentang 0 hingga 1, sehingga cocok dengan pola XOR yang memang membutuhkan pembeda non-linear untuk memisahkan kelasnya. Selain itu, dua neuron pada hidden layer sudah menjadi jumlah minimal yang cukup untuk membentuk representasi XOR, sehingga jaringan dapat belajar secara efektif tanpa mengalami kesulitan dalam pembaruan gradien.\n",
    "\n",
    "Sementara itu, pada percobaan kedua, jaringan menggunakan ReLU dengan tiga neuron pada hidden layer. Meskipun jumlah neuronnya lebih banyak, loss berkisar pada angka 0.1666 dan tidak mengalami penurunan signifikan meskipun dilakukan training hingga 10.000 epoch. Hasil prediksi juga terlihat salah pada hampir seluruh pola XOR. Penyebabnya terletak pada sifat ReLU yang hanya aktif pada nilai positif, sehingga ketika input jaringan menghasilkan nilai negatif atau nol pada neuron tertentu, gradiennya menjadi nol dan neuron tersebut berhenti belajar. Dalam kasus XOR yang memiliki pola masukan kecil seperti `0` dan `1`, banyak aktivasi ReLU justru bernilai nol, sehingga jaringan gagal membedakan pola non-linearnya.\n",
    "\n",
    "Dengan kata lain, meskipun ReLU sangat efektif pada jaringan besar maupun data berskala besar, pada masalah sederhana dan bernilai kecil seperti XOR, sigmoid lebih stabil dan mampu membentuk non-linearitas yang dibutuhkan. Hal tersebut menjadikan konfigurasi pertama lebih sukses dalam menyelesaikan XOR dibandingkan konfigurasi kedua meski memiliki lebih sedikit neuron.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
